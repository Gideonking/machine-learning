{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Split into Train and Test Sets:\n",
    "The simplest method that we can use to evaluate the performance of a machine learning\n",
    "algorithm is to use different training and testing datasets. We can take our original dataset and\n",
    "split it into two parts. Train the algorithm on the first part, make predictions on the second\n",
    "part and evaluate the predictions against the expected results. The size of the split can depend\n",
    "on the size and specifics of your dataset, although it is common to use 67% of the data for\n",
    "training and the remaining 33% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  75.5905511811\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using a train and a test set\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = 'pima.csv'\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "\n",
    "print(\"Accuracy: \", result*100.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation:\n",
    "Cross validation is an approach that you can use to estimate the performance of a machine\n",
    "learning algorithm with less variance than a single train-test set split. It works by splitting\n",
    "the dataset into k-parts (e.g. k = 5 or k = 10). Each split of the data is called a fold. The\n",
    "algorithm is trained on k âˆ’ 1 folds with one held back and tested on the held back fold. This is\n",
    "repeated so that each fold of the dataset is given a chance to be the held back test set. After\n",
    "running cross validation you end up with k different performance scores that you can summarize\n",
    "using a mean and a standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  (76.951469583048521, 4.8410519245671946)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using Cross Validation\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = 'pima.csv'\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: \", (results.mean()*100.0, results.std()*100.0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we report both the mean and the standard deviation of the performance\n",
    "measure. When summarizing performance measures, it is a good practice to summarize the\n",
    "distribution of the measures, in this case assuming a Gaussian distribution of performance (a\n",
    "very reasonable assumption) and recording the mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Leave One Out Cross Validation:\n",
    "You can configure cross validation so that the size of the fold is 1 (k is set to the number of\n",
    "observations in your dataset). This variation of cross validation is called leave-one-out cross\n",
    "validation. The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data.\n",
    "A downside is that it can be a computationally more expensive procedure than k-fold cross\n",
    "validation. In the example below we use leave-one-out cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using Leave One Out Cross Validation\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = ' pima.csv '\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "loocv = LeaveOneOut()\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=loocv)\n",
    "print(\"Accuracy: \", (results.mean()*100.0, results.std()*100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
