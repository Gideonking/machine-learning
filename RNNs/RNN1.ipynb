{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "# from keras.utils.visualize_util import plot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read our input text from the text of Alice in Wonderland on the Project Gutenberg website ( http://www.gutenberg.org/files/11/11-0.txt ). The file contains line breaks and non-ASCII characters, so we do some\n",
    "preliminary cleanup and write out the contents into a variable called text :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open('alice.txt', 'rb')\n",
    "lines = []\n",
    "for line in fin:\n",
    "    line = line.strip().lower() #Convert every line to lowercase\n",
    "    line = line.decode(\"ascii\", \"ignore\")\n",
    "    if len(line) == 0: continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are building a character-level RNN, our vocabulary is the set of characters that occur in the\n",
    "text. There are 42 of them in our case. Since we will be dealing with the indexes to these characters\n",
    "rather than the characters themselves, the following code snippet creates the necessary lookup tables:\n",
    "The next step is to create the input and label texts. We do this by stepping through the text by a numberof characters given by the STEP variable ( 1 in our case) and then extracting a span of text whose size is\n",
    "determined by the SEQLEN variable ( 10 in our case). The next character after the span is our label\n",
    "character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "char2index = dict((c,i) for i, c in enumerate(chars))\n",
    "index2char = dict((i,c) for i, c in enumerate(chars))\n",
    "\n",
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to vectorize these input and label texts. Each row of the input to the RNN\n",
    "corresponds to one of the input texts shown previously. There are SEQLEN characters in this input, and\n",
    "since our vocabulary size is given by nb_chars , we represent each input character as a one-hot encoded\n",
    "vector of size ( nb_chars ). Thus each input row is a tensor of size ( SEQLEN and nb_chars ). Our output label\n",
    "is a single character, so similar to the way we represent each character of our input, it is represented\n",
    "as a one-hot vector of size ( nb_chars ). Thus, the shape of each label is nb_chars :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False, input_shape=(SEQLEN, nb_chars),\n",
    "    unroll=True))\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training approach is a little different from what we have seen so far. So far our approach has\n",
    "been to train a model for a fixed number of epochs, then evaluate it against a portion of held-out test\n",
    "data. Since we don't have any labeled data here, we train the model for an epoch\n",
    "( NUM_EPOCHS_PER_ITERATION=1 ) then test it. We continue training like this for 25 ( NUM_ITERATIONS=25 ) iterations,\n",
    "stopping once we see intelligible output. So effectively, we are training for NUM_ITERATIONS epochs and\n",
    "testing the model after each epoch.\n",
    "Our test consists of generating a character from the model given a random input, then dropping the\n",
    "first character from the input and appending the predicted character from our previous run, and\n",
    "generating another character from the model. We continue this 100 times ( NUM_PREDS_PER_EPOCH=100 ) and\n",
    "generate and print the resulting string. The string gives us an indication of the quality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 25s 156us/step - loss: 2.3396\n",
      "Generating from seed: aid the ca\n",
      "aid the case the wast ou the wast ou the wast ou the wast ou the wast ou the wast ou the wast ou the wast ou t==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 24s 149us/step - loss: 2.0561\n",
      "Generating from seed: ded again.\n",
      "ded again. the was in the wast on the wast on the wast on the wast on the wast on the wast on the wast on the ==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 27s 167us/step - loss: 1.9530\n",
      "Generating from seed: ht thing t\n",
      "ht thing the more the she had the wast of the the the what she was so the the har said the mouth the mouth the==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 32s 199us/step - loss: 1.8696\n",
      "Generating from seed: the corner\n",
      "the cornerse the mouse of the parse the mouse of the parse the mouse of the parse the mouse of the parse the m==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 31s 192us/step - loss: 1.80082s -\n",
      "Generating from seed: e youre tr\n",
      "e youre trought of the could alice and and the could alice and and the could alice and and the could alice and==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 34s 216us/step - loss: 1.7442\n",
      "Generating from seed:  see parag\n",
      " see paraged the dormouse the doon and her alice found the dormouse the doon and her alice found the dormouse ==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 35s 221us/step - loss: 1.6972\n",
      "Generating from seed: ed tone. t\n",
      "ed tone. the could not the doon and the could not the doon and the could not the doon and the could not the do==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 36s 229us/step - loss: 1.6573\n",
      "Generating from seed: turning to\n",
      "turning to the dormouse of the gryphon and the dormouse of the gryphon and the dormouse of the gryphon and the==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 37s 233us/step - loss: 1.6235\n",
      "Generating from seed: at the sti\n",
      "at the stime the gryphon. i dont the gryphon. i dont the gryphon. i dont the gryphon. i dont the gryphon. i do==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 37s 230us/step - loss: 1.5949\n",
      "Generating from seed: r computer\n",
      "r computers with a must it was to the parter the said alice as the cat had to the door the door the door the d==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 37s 231us/step - loss: 1.5703\n",
      "Generating from seed: hould unde\n",
      "hould under the for of the catter with the the hatter with the the hatter with the the hatter with the the hat==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 39s 249us/step - loss: 1.5482\n",
      "Generating from seed: equire suc\n",
      "equire such a sures in a mory the cats and the pabled it was she had the parted the project gutenberg-tm elect==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 40s 255us/step - loss: 1.5274\n",
      "Generating from seed: ould like \n",
      "ould like the dormouse with the dormouse with the dormouse with the dormouse with the dormouse with the dormou==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 33s 206us/step - loss: 1.5117\n",
      "Generating from seed: she had dr\n",
      "she had dread they had not the was a little see car out of the cat her her her her her her her her her her her==================================================\n",
      "Iteration #: 14\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 35s 217us/step - loss: 1.4958\n",
      "Generating from seed:  same litt\n",
      " same little begin she had the little begin she had the little begin she had the little begin she had the litt==================================================\n",
      "Iteration #: 15\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 29s 184us/step - loss: 1.4808\n",
      "Generating from seed:  freely sh\n",
      " freely she was to all the eldere to see in a mores to herself alice was the some to see in a mores to herself==================================================\n",
      "Iteration #: 16\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 31s 197us/step - loss: 1.4694\n",
      "Generating from seed: rather unw\n",
      "rather unwind she had nothing it was so and the thing the parting a good the mork the hatter with a creat of t==================================================\n",
      "Iteration #: 17\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 43s 270us/step - loss: 1.4576\n",
      "Generating from seed: s paper ha\n",
      "s paper hat she had to herself and the some the mock turtle sous of the some the mock turtle sous of the some ==================================================\n",
      "Iteration #: 18\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 45s 283us/step - loss: 1.4476\n",
      "Generating from seed: ied the mo\n",
      "ied the mock turtle sous to any pares the ratespiting out of the gryphon the rabbit was and and heard the mock==================================================\n",
      "Iteration #: 19\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 43s 269us/step - loss: 1.43770s \n",
      "Generating from seed: thats the \n",
      "thats the parters the caterpillar the was a little she was a little she was a little she was a little she was ==================================================\n",
      "Iteration #: 20\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 44s 277us/step - loss: 1.4282\n",
      "Generating from seed: .  except \n",
      ".  except some of the project gutenberg-tm electronic works you have a little door and all the project gutenbe==================================================\n",
      "Iteration #: 21\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 45s 282us/step - loss: 1.4195\n",
      "Generating from seed: nd as the \n",
      "nd as the sormations to be a little she want the white rabbit alice was not as she was not as she was not as s==================================================\n",
      "Iteration #: 22\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 46s 288us/step - loss: 1.4125\n",
      "Generating from seed: reasonable\n",
      "reasonable the door all the poor a little said the king the project gutenberg-tm electronic works the poor a l==================================================\n",
      "Iteration #: 23\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 46s 292us/step - loss: 1.4047\n",
      "Generating from seed: t i havent\n",
      "t i havent in a little seemed to the beat her head to see have to make the hatter was a little seemed to the b==================================================\n",
      "Iteration #: 24\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 42s 265us/step - loss: 1.3966\n",
      "Generating from seed:  of this a\n",
      " of this agreement any parter. there was a little good dear the caterpillar that she was no really any parter.\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))bb\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    \n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    print(\"Generating from seed: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            Xtest[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(Xtest, verbose=0)[0]\n",
    "        ypred = index2char[np.argmax(pred)]\n",
    "        print(ypred, end=\"\")\n",
    "        # move forward with test_chars + ypred\n",
    "        test_chars = test_chars[1:] + ypred\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(model, to_file='model.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
